
# 量化
https://zhuanlan.zhihu.com/p/11886909512


## LLM.int8()、GPTQ
https://zhuanlan.zhihu.com/p/680212402
* 大模型量化的对象主要有：权重、激活、KV Cache、梯度、优化器等

1. 梯度量化主要在训练场景使用，用于减少反向传播时的计算和通信开销
2. 优化器量化（如：8-Bit Optimizers Via Block-Wise Quantization）也是用于训练场景
3. 仅权重量化，如：W4A16、AWQ及GPTQ中的W4A16，W8A16（权重量化为INT8，激活仍为BF16或FP16）
4. 权重、激活量化，如：SmoothQuant中的W8A8
5. KV Cache INT8 量化

### LLM.int8()
LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale
这些离群值分布在少量的几个特征中，称为离群特征 (Emergent Features)
LLM.in8() 的思路是把这些特征拿出来单独计算，只对剩余特征做量化。
该方案先做了一个矩阵分解，对绝大部分权重和激活用8bit量化（vector-wise）。对离群特征的几个维度保留16bit，对其做高精度的矩阵乘法。
![alt text](./img/量化/image-LLM-int8-1.png)

* 通过三个步骤完成矩阵乘法计算:
1. 从输入的隐含状态中，按列提取异常值 (离群特征，即大于某个阈值的值)。
2. 对离群特征进行 FP16 矩阵运算，对非离群特征进行量化，做 INT8 矩阵运算；
3. 反量化非离群值的矩阵乘结果，并与离群值矩阵乘结果相加，获得最终的 FP16 结果。

虽然 LLM.in8() 带来的性能下降微乎其微，但是这种分离计算的方式拖慢了推理速度。
* Transformer 中受模型大小或 C4 困惑度影响的大量异常值特征的层和所有序列维度的百分比。
当通过困惑度（perplexity）进行测量时，Transformer 所有层中大量异常值特征的出现可以被视为根据困惑度递减的指数函数平滑的出现。


异常值的出现不仅与模型大小有关，还与困惑度有关，而困惑度与多个其他因素有关，例如：使用的训练数据量和数据质量。

* 在 Transformers 中使用 LLM.int8() 只需提前安装 bitsandbytes 即可，使用 LLM.int8() 方法量化transformer模型具体示例如下
```
# 8bit量化：
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(
  'decapoda-research/llama-7b-hf',
  device_map='auto',
  load_in_8bit=True,
  max_memory={
    i: f'{int(torch.cuda.mem_get_info(i)[0]/1024**3)-2}GB'
    for i in range(torch.cuda.device_count())
  }
)
# 4bit量化：
from transformers import BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
   load_in_4bit=True,
   bnb_4bit_quant_type="nf4",
   bnb_4bit_use_double_quant=True,
   bnb_4bit_compute_dtype=torch.bfloat16
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)

```

### GPTQ
GPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PR E-TRAINED TRANSFORMERS
原理来自于另一个量化方法OBQ, 而OBQ 实际上是对 OBS(Optimal Brain Surgeon，一种比较经典的剪枝方法）的魔改， 而OBS则来自于OBD（Optimal Brain Damage，由 Yann LeCun 在1990年提出的剪枝方法）


1. 采用 int4/fp16 (W4A16) 的混合量化方案，其中模型权重被量化为 int4 数值类型，而激活值则保留在 float16，是一种仅权重量化方法
2. 在推理阶段，模型权重被动态地反量化回 float16 并在该数值类型下进行实际的运算
3. 同 OBQ 一样，GPTQ还是从单层量化的角度考虑，希望找到一个量化过的权重，使的新的权重和老的权重之间输出的结果差别最小。

通过求解海森矩阵的逆，就可以计算每个参数权重对目标的影响

GPTQ 将权重分组（如：128列为一组）为多个子矩阵（block）。对某个 block 内的所有参数逐个量化，每个参数量化后，需要适当调整这个 block 内其他未量化的参数，以弥补量化造成的精度损失。因此，GPTQ 量化需要准备校准数据集。
首先，使用 Cholesky 分解中 Hessian 矩阵的逆，
其中XX^T是对应于优化问题的Hessian矩阵
![alt text](./img/量化/image-Hessian-1.png)
在给定的step中对连续列的块（粗体）进行量化，并在step结束时更新剩余的权重（蓝色）。量化过程在每个块内递归应用：白色中间列表示当前正在被量化。

![alt text](./img/量化/image-GPTQ-1.png)

* GPTQ 的创新点如下
1. 取消贪心算法
2. Lazy Batch-Updates
3. Cholesky 分解
![alt text](./img/量化/image-GPTQ-2.png)


* Llama 量化
https://github.com/qwopqwop200/GPTQ-for-LLaMa
https://github.com/turboderp/exllama
https://github.com/ggerganov/llama.cpp/

* Transformers 量化
https://github.com/AutoGPTQ/AutoGPTQ

```
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig

model_id = "facebook/opt-125m"
tokenizer = AutoTokenizer.from_pretrained(model_id)
quantization_config = GPTQConfig(bits=4, dataset = "c4", tokenizer=tokenizer)

model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=quantization_config)
```


## SmoothQuant
SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models
* 背景
LLM.int8() 发现当 LLMs 的模型参数量超过 6.7B 的时候，激活中会成片的出现大幅的离群点(outliers)，朴素且高效的量化方法（W8A8、ZeroQuant等）会导致量化误差增大，精度下降。
新出现的离群特征（Emergent Features）的分布是有规律的。通常，这些离群特征只分布在 Transformer 层的少数几个维度。
针对这个问题，LLM.int8() 采用了混合精度分解计算的方式


* 大模型量化困难的原因，总结下来就三点：
1. 激活比权重更难量化（之前的工作LLM.int8()表明，使用 INT8 甚至 INT4 量化 LLM 的权重不会降低准确性。）
2. 异常值让激活量化更困难（激活异常值比大多数激活值大约 100 倍。 如果我们使用 INT8 量化，大多数值将被清零。）
3. 异常值持续存在于固定的通道（channel）中（固定通道存在异常值，并且异常值通道值较大）

量化方法可以分为逐层量化（per-tensor）、逐通道（per-token & per-channel 或者 vector-wise quantization ）量化和逐组量化（per-group、Group-wise）。
![alt text](./img/量化/image-SmoothQuant-1.png)


SmoothQuant 提出了一种数学上等价的逐通道缩放变换（per-channel scaling transformation），可显著平滑通道间的幅度，从而使模型易于量化，保持精度的同时，还能够保证推理提升推理速度

* SmoothQuant
由于权重很容易量化，而激活则较难量化，因此，SmoothQuant 引入平滑因子s来平滑激活异常值，通过数学上等效的变换将量化难度从激活转移到权重上。

![alt text](./img/量化/image-SmoothQuant-2.png)

Y=XW
SmoothQuant 对激活进行 smooth，按通道除以 smoothing factor。为了保持线性层数学上的等价性，以相反的方式对权重进行对应调整。
Y=(Xdiag(s)^−1)⋅(diag(s)W)= XW

* 将量化难度从激活迁移到权重
1. sj=max(∣Xj∣)
各 channel 通过除以sj后，激活 channels 都将有相同的最大值，这时激活比较容易量化。但是这种做法会把激活的量化难度全部转向权重，导致一个比较大的精度损失。
2. sj=1/max(∣Xj∣)
这样权重 channels 都将有相同的最大值，权重易量化，但激活量化误差会很大。

本文作者通过加入一个超参 α 迁移强度），来控制从激活值迁移多少难度到权重值。一个合适的迁移强度值能够让权重和激活都易于量化。α 太大，权重难以量化，α 太小激活难以量化。


当 α 为 0.5 时 SmoothQuant 的主要思想
![alt text](./img/量化/image-SmoothQuant-3.png)

1. 校准阶段（离线）

![alt text](./img/量化/image-SmoothQuant-4.png)
![alt text](./img/量化/image-SmoothQuant-5.png)

2. 平滑阶段（离线）
![alt text](./img/量化/image-SmoothQuant-6.png)

3. 推理阶段（在线，部署模型）
![alt text](./img/量化/image-SmoothQuant-7.png)

作者使用 CUTLASS INT8 GEMM 内核为 PyTorch 实现 SmoothQuant 真正的 INT8 推理，这些内核被包装为 torch-int 中的 PyTorch 模块。加载SmoothQuant量化后的模型示例如下
```
from smoothquant.opt import Int8OPTForCausalLM

def print_model_size(model):
    # https://discuss.pytorch.org/t/finding-model-size/130275
    param_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    buffer_size = 0
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()

    size_all_mb = (param_size + buffer_size) / 1024**2
    print('Model size: {:.3f}MB'.format(size_all_mb))

model_smoothquant = Int8OPTForCausalLM.from_pretrained(
    'mit-han-lab/opt-30b-smoothquant', torch_dtype=torch.float16, device_map='auto')
print_model_size(model_smoothquant)
```


## AWQ、AutoAWQ
AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration
* 背景
量化感知训练（QAT）由于训练成本较高并不实用，而训练后量化（PTQ）在低比特场景下面临较大的精度下降。
最接近的工作是GPTQ，它使用二阶信息来进行误差补偿，但它可能在重建过程中过拟合校准集，从而扭曲分布之外领域上的学习特征，这可能会出现问题，因为 LLM 是通才模型。

### AWQ
对大模型仅权重量化方法

![alt text](./img/量化/image-AWQ-1.png)

1. 通过保留1%的显著权重来改进LLM量化
通过激活感知缩放保护显著权重:通过按逐通道（per-channel）缩放来减少显著权重的量化误差，这种方法不存在硬件效率低下的问题。

![alt text](./img/量化/image-AWQ-2.png)



### AutoAWQ
AutoAWQ 是一个易于使用的 4 比特量化模型包。 与 FP16 相比，AutoAWQ 将模型速度提高了 3 倍，并将对内存需求降低了 3 倍。 AutoAWQ 实现激活感知权重量化 (AWQ) 算法来量化 LLM。


## SpQR

SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression

一种混合稀疏量化格式，其工作原理是识别和隔离异常值权重，并以更高的精度存储它们，同时将所有其他权重压缩到3-4位，并实现小于1%的精度损失

* SpQR 提供了高效的算法，可以将权重编码为 SpQR 格式，并在运行时对其进行高效解码

为了将给定的预训练LLM转换为SpQR格式，作者采用了GPTQ的扩展版本。该方法通过未压缩模型传递校准数据；为了压缩每一层，它针对未压缩模型的输出和量化权重的输出之间的 L2 误差应用逐层（layer-wise）求解器。
两个步骤
1. 异常值检测步骤，隔离直接量化对层输出行为产生巨大影响的权重，
2. 实际压缩步骤，大多数（≥ 99%）权重被压缩到低位宽。


本文首次证明，类似的异常值出现在权重(对于特定输出隐藏维度)中。与输入特征异常值不同，输出隐藏维度异常值仅出现在特定输出隐藏维度的小片段中。

1. 作者提出的量化算法隔离此类异常值，并以 SpQR 格式有效地对给定模型进行编码
2. 为了利用所得结构，还开发了一种基于压缩稀疏行（CSR）格式的专门稀疏矩阵乘法算法
3. 为了使用 SpQR 进行逐个token生成，我们将这种稀疏算法与 3-4 比特权重的密集量化矩阵乘法结合起来FF

* sij 灵活度
权重矩阵W某个权重wij

![alt text](./img/量化/image-SpQR-1.png)

Hessian矩阵
![alt text](./img/量化/image-Hessian-1.png)


权重矩阵中敏感权重的位置不是随机的，而是具有特定的结构
下图可视化了 LLaMA-65B 最后一个自注意力层的输出投影的权重敏感度。

![alt text](./img/量化/image-SpQR-2.png)



* 以注意力权重矩阵为例，对异常值结构进行分类：
![alt text](./img/量化/image-SpQR-3.png)
作者将利用这些发现提出一个压缩表示，可以支持所有这些不同的离群值类型。


* SpQR 技术原理
SpQR确定并隔离了异常权重，将其存储在更高的精度中，并将所有其他权重压缩为3-4比特。具体工作原理如下：
1. 首先，我们隔离离群权重，我们发现其量化会导致不成比例的高误差。因此，将这些权重保持高精度，而其他权重存储在低得多的精度中，例如：3比特格式。
2. 其次，我们实现了一种具有非常小的组大小（group size）的分组量化的变体，例如：16 个连续元素，但我们将量化缩放（scales）本身量化为 3 比特表示。
然而，通过上面的讨论表明这可能会导致次优量化。理想情况下，将表示将更多的大小预算（size budge）分配给敏感权重。然而，这些权重作为单独权重或小组（small groups）分散在权重矩阵中，例如：部分行或注意力头。
为了捕获这种结构，我们对量化过程进行了两项更改：
1. 通过双层量化捕获小组（small groups ）权重
有效地解决了权重在连续小组中表现相似但组间突然变化的问题（例如某些注意力头和部分行异常值）。
第一层量化：对每 β 个连续权重进行分组量化，计算每组的 scale 和 zero-point。
第二层量化：对分组统计数据（Scale（缩放因子） 和 Zero-Point（零点偏移））本身进行量化。
使用与权重相同的量化算法（非对称最小-最大量化）来量化分组统计数据。
由于最小-最大量化的特性，量化值的范围会适应具有最大（或最小）量化 scale 的组，从而完美地量化它们。
2. 高灵敏度异常值。

按行排列编码：采用按行排列的方式对敏感权重进行单独编码，类似于 CSR 表示，高效处理非结构化异常值和小结构。

左侧片段描述了完整的过程，右侧包含用于bilevel量化和查找异常值的子例程。

![alt text](./img/量化/image-SpQR-4.png)

* 稀疏量化表示的实现和利用
将同质权重转换为不同大小和精度的多种数据结构。
（1）量化权重、（2）第一层量化量化统计数据、第二层量化统计数据、（3）CSR异常值指数和值组成。

![alt text](./img/量化/image-SpQR-5.png)

1. 存储量化组。所有非异常值权重均被编码为包含以下内容的结构：
bw比特独立的权重；
每组大小为 B 的 bq 比特 scale 和 zero point；
用于量化Bq量化组（scale 和 zero point）的 16 比特统计数据。
2. 存储异常值。
对于每个异常值，存储两个标量：
16 比特权重值：异常值的具体数值。
16 比特列索引：异常值所在的列位置
对于每一行，存储一个 32 比特数字：表示到当前行为止的行中异常值的总数，用于高效推理。


* 使用 SpQR 进行推理
我们的算法将组统计数据和量化权重加载到共享内存 (SRAM) 中，反量化为 16 位，然后与 16 位输入执行矩阵乘法。为了处理异常值，我们设计了一种稀疏矩阵算法，该算法利用行中出现的异常值。粗略地说，该算法的工作原理如下：
1. 首先，我们将矩阵划分为大小相等的块(block)。
2. 然后，每个 GPU core（线程块block）将一大片异常值加载到共享内存 (SRAM) 中。
3. 并且，每个 GPU core 确定异常值是否是该段的一部分。
4. 之后从主存加载相应的权重；最后进行矩阵乘法。

