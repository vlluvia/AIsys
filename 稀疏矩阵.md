
## Quest

https://arxiv.org/pdf/2406.10774

随着对长上下文LLMs需求的增加，模型的上下文窗口已经扩展到128K或1M个token。   
。然而，长上下文LLMs的推理速度会随着序列长度的增加而显著下降，这主要是由于在自注意力（self-attention）阶段需要加载大量的KV缓存。
以往的研究表明，一小部分关键token在注意力结果中占主导地位，但文章观察到token的关键性高度依赖于查询（query）。

* quest
Quest算法通过跟踪KV缓存页面中的最小和最大Key值，并使用查询向量来估计给定页面的关键性，从而只加载Top-K个关键KV缓存页面进行注意力计算

以下是为什么要使用最小值和最大值的原因：
1. 确定上下界
2. 近似最高注意力权重

* 例如
Token A: Key = 2
Token B: Key = -1
Token C: Key = 3
Token D: Key = 0


对于最小Key值：U_min = Q * Min Key = 2 * (-1) = -2
对于最大Key值：U_max = Q * Max Key = 2 * 3 = 6

![alt text](./img/稀疏性/image-quest-1.png)


## RetrievalAttention
https://arxiv.org/pdf/2409.10516
基于动态稀疏注意力的长文本LLM推理加速方法

该方法将大部分键值向量卸载到 CPU 内存，并通过向量搜索实现动态稀疏注意力机制，从而在保证准确性的同时显著降低了计算成本和 GPU 内存占用


现有的加速长上下文 LLM 推理的尝试集中于利用注意力稀疏性来压缩 KV 缓存大小。通常会导致精度显着下降。

FlexGen 和 Lamina，将 KV 缓存卸载到 CPU 内存，但在缓慢且成本高昂的全注意力计算方面面临挑战
Quest 和 InfLLM，将 KV 缓存划分为块并选择代表性关键向量，但其有效性在很大程度上取决于这些代表的准确性
SparQ、InfiniGen 和 LoKi 尝试通过减少头维度来逼近最相关的 top-k 键。

* RetrievalAttention

一种旨在加速长上下文 LLM 生成的创新方法
1. 在 token 生成期间采用动态稀疏注意力，允许从广泛的上下文数据中出现最关键的 token
2. 为了解决 out-of-distribution (OOD) 挑战，RetrievalAttention 引入了一个专门为注意力机制量身定制的向量索引，专注于查询分布而不是关键相似性

3. RetrievalAttention 还通过在 GPU 内存中保留少量遵循静态模式的 KV 向量来优化 GPU 内存消耗，同时将大部分 KV 向量卸载到 CPU 内存以进行索引构建
4. token 生成期间，它使用 CPU 上的向量索引有效地检索关键 token，并合并来自 CPU 和 GPU 的部分注意力结果

![alt text](./img/稀疏性/image-RetrievalAttention-1.png)
RetrievalAttention 采用 CPU-GPU 协同执行策略来加速长上下文 LLM 推理的注意力计算
该方法将注意力计算分解为两组不相交的 KV 缓存向量：GPU 上的可预测向量和 CPU 上的动态向量。

它利用在预填充阶段观察到的模式来预测 token 生成期间持续激活的 KV 向量，并将它们保留在 GPU 缓存中。当前的实现使用固定的初始 token 和上下文的最后一个滑动窗口作为静态模式，类似于 StreamingLLM。

对于 CPU 端计算，RetrievalAttention 构建了一个注意力感知向量搜索索引，以有效地检索相关的 KV 向量。该索引是使用从查询向量到关键向量的 KNN 连接构建的，然后将其投影到关键向量上以简化搜索过程。


RetrievalAttention 在组合 CPU 和 GPU 组件的注意力结果之前独立计算它们，其灵感来自 FastAttention。

## Post-Training Sparse Attention with Double Sparsity

https://arxiv.org/abs/2310.17157


旨在解决大型语言模型（LLMs）在推理过程中速度慢和内存密集的问题，特别是减少关键值（KV）缓存访问
Double Sparsity结合了令牌稀疏性和通道稀疏性，通过仅使用重要的令牌和特征通道来计算自注意力

* Double Sparsity
1. 模型同样接收一个包含1000个词的序列。
2. 通过离线校准，Double Sparsity识别出对于每个词来说最重要的特征通道，假设我们发现只有16个通道是重要的。
3. 在运行时，模型只关注这些重要的通道，而不是所有通道，从而减少了对KV缓存的访问。
4. 进一步地，Double Sparsity还识别出序列中最重要的词（令牌），假设我们发现只有64个词是重要的。
5. 模型只计算这64个重要词的注意力权重，而不是所有1000个词。


* 离线校准过程

1. 在模型训练完成后，我们选择一个小的验证集，比如包含100个句子的集合。
2. 我们运行模型，让模型处理这个验证集，并记录每个特征通道对模型输出的贡献度。
3. 通过分析，我们发现在这些计算中，只有20%的特征通道对模型的最终决策有显著影响。
4. 我们标记这些重要的通道，并记录它们的索引或标识。
![alt text](./img/稀疏性/image-Double-Sparsity-1.png)


## ClusterKV

https://arxiv.org/pdf/2412.03213
现有的KV缓存压缩方法要么永久性地驱逐令牌（tokens），要么以页面为单位召回先前的令牌，这两种方法都会降低模型的准确性和输出质量


ClusterKV通过在语义空间中对令牌进行聚类，然后仅计算与聚类表示相关的注意力权重，而不是单个令牌，从而显著减少召回开销。
论文提出了一个简单的K-means算法来对键向量进行聚类，并在聚类基础上进行选择，以确定对给定查询最重要的令牌。


* 算法
1. ClusterKV在预填充（prefill）阶段在GPU上处理键张量，生成聚类中心和相应的元数据，并将生成的KV张量卸载到CPU内存。
2. 在解码阶段，根据查询向量和聚类中心的注意力权重来确定每个聚类的重要性，并使用这些结果以及聚类元数据来生成选定令牌的索引。
3. ClusterKV还维护了一个在GPU上的聚类粒度缓存，以存储选定令牌的KV，减少从CPU内存到GPU内存的不必要数据传输，提高性能。
![alt text](./img/稀疏性/image-ClusterKV-1.png)

![alt text](./img/稀疏性/image-ClusterKV-2.png)

## LORC（Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy）

现有的方法包括在模型升级阶段集成高效的注意力变体，这需要大量的参数调整，不适合预训练的LLMs。
测试时的KV缓存压缩主要通过令牌驱逐策略实现，这些方法通常忽视了层间的依赖关系，可能是任务特定的。

* LORC方法
1. 提出了一种基于KV权重矩阵的低秩近似方法，允许与现有的基于变换器的LLMs插件集成，无需模型重新训练。
2. 通过调整层间敏感性并引入渐进式压缩策略，有效压缩KV缓存。
3. 该方法不需要在升级阶段进行模型调整或在测试阶段进行任务特定的分析。


* 低秩近似
我们发现，很多书籍（KV向量）之间存在相似性（低秩特性），这意味着我们可以用更少的书籍来代表它们。
例如，如果我们有100本书，但它们都是10本不同书籍的副本，那么我们只需要记住这10本不同的书籍，而不是100本。

* 渐进式压缩策略
我们不是简单地从每个书架上移除相同数量的书籍，而是根据每个书架的重要性来决定保留多少书籍。
比如，靠近入口的书架（浅层）可能更常用，所以我们保留更多的书籍；而深入图书馆的书架（深层）可以保留较少的书籍，因为它们不常被访问。

* 理论分析
我们分析了如果从某个书架上移除太多书籍，可能会影响后续书架上书籍的查找效率（误差传播）。
因此，我们根据每个书架的重要性（累积条件数）来决定压缩的程度


![alt text](./img/稀疏性/image-LORC-1.png)
通过SVD分解来压缩KV缓存，并通过渐进式压缩策略在不同层级上应用不同程度的压缩，以平衡模型性能和内存效率。


SVD，即奇异值分解（Singular Value Decomposition），是一种在线性代数中用于分解矩阵的数学技术。它将一个矩阵分解为一系列特定的矩阵乘积

