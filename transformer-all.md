

## 静态 Token 稀疏化

### Dense Attention

### Window Attention

### Sliding Window Attention with recomputation

### StreamingLLM

---
## 动态 Token 稀疏化

### H2O

### Quest

---
##  Prefix Caching 

### Radix Attention

### Cascade Inference

---
## KV Cache

### Layers

#### KV Cache

#### LayerSkip

#### YOCO

### HeadDim & NumHeads & HiddenSize

#### MQA、GQA 和 MLA
DeepSeek V2

### DataType

#### KIVI

#### Per-Group

---
## Flash Attention

## Chunk Prefills


## Page Attention
https://arxiv.org/abs/2309.06180
https://github.com/vllm-project/vllm
* 问题
![Alt text](img/transformer/image-pa-1.png)

* 架构
![Alt text](img/transformer/image-pa-2.png)

![Alt text](img/transformer/image-pa-3.png)


* Feature
![Alt text](img/transformer/image-pa-4.png)

![Alt text](img/transformer/image-pa-5.png)


## faster-transformer

* v1

https://github.com/NVIDIA/FasterTransformer



* v2

https://github.com/NVIDIA/FasterTransformer/tree/v2.0/

https://zhuanlan.zhihu.com/p/650462095


## TurboTransformers

https://github.com/Tencent/TurboTransformers








