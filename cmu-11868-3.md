# cmu-11868

## GPTQ


### Related Work

* AdaQuant

* BRECQ

* ZeroQuant

* LLM.int8()



## LoRA & QLoRA

###  Motivation & Related works (PEFT)

* Full-parameter Fine-Tuning
> Update all model parameters

* Parameter Efficient Fine-tuning (PEFT)
>  Only update a small subset of parameters, but not degrade the quality of the model

1. Lora 
Weight: 16 bits 
65B model -> 143 GB of GPU memory -> 4x data center GPUs (8x consumer GPUS)
2. QLoRA
Weight: 4 bits
65B model -> 780GB 42 GB of GPU memory -> 17x 1x data center GPUs


*  PEFT

![alt text](img/cmu-11868-3/image-1.png)
![alt text](img/cmu-11868-3/image-3.png)
1. Prompt tuning (Lester et al. 2021)
![alt text](img/cmu-11868-3/image-2.png)

2. Prefix tuning  (Li and Liang 2021)
![alt text](img/cmu-11868-3/image-4.png)

3. Adapter (Houlsby et al. 2019)
![alt text](img/cmu-11868-3/image-5.png)


### LORA
> We can hypothesize that the update matrices in LM adaptation have a
low “intrinsic rank”, leading to Low-Rank Adaptation (LoRA)
![alt text](img/cmu-11868-3/image-6.png)

![alt text](img/cmu-11868-3/image-7.png)


### QLoRA
![alt text](img/cmu-11868-3/image-8.png)

* 主要贡献
1. 4-bit Normal Float
2. Double Quantization
3. Page Optimizer
4. 4-bit storage data type
5. Bfloat16 computational data type


#### 模型量化
1. Post-Training Quantization (PTQ):
2. Quantization-Aware Training (QAT): QLoRA

* Floating-point numbers
![alt text](img/cmu-11868-3/image-9.png)

* FP32 ->  Int8 [-127,127]
![alt text](img/cmu-11868-3/image-10.png)


* 4-bit Normal Float Quantization
> 预训练LLM 通常以0为正态分布

NF-4 (k = 4)过程
(1) Estimate the 16 + 1 quantiles of a theoretical N(0, 1) distribution
(2) Normalized its value into [-1, 1] range
(3) Quantize the input weight tensor into [-1, 1] range

![alt text](img/cmu-11868-3/image-11.png)

![alt text](img/cmu-11868-3/image-12.png)

* Double Quantization
![alt text](img/cmu-11868-3/image-13.png)


* Paged Optimizers
![alt text](img/cmu-11868-3/image-14.png)


## Zero


* Tensor Parallelism: Megatron-LM
* Pipeline Parallelism: GPipe\PipeDream
* Data Parallelism

1. Naïve AllReduce
> N workers, each M params, overall N * (N-1) * M params
![alt text](img/cmu-11868-3/image-15.png)
2. Ring AllReduce 
> each worker send one slice (M/N parameters) to the next worker on the ring; repeat N times
> each worker send one slice (M/N parameters) to the next worker on the ring; repeat N times
> each worker send one slice (M/N parameters) to the next worker on the ring; repeat N times
> After step 1, each worker has the aggregated version of M/N parameters
> each worker send one slice of aggregated parameters to the next worker; repeat N times

![alt text](img/cmu-11868-3/image-16.png)
![alt text](img/cmu-11868-3/image-19.png)
![alt text](img/cmu-11868-3/image-17.png)
![alt text](img/cmu-11868-3/image-18.png)


#### Memory Usage
> GPU 需要存储 model weights, forward activation, backward gradient, optimizer state
* 常规方法：Adam + Mixed-precision
![alt text](img/cmu-11868-3/image-20.png)

#### Memory Consumption
![alt text](img/cmu-11868-3/image-21.png)


#### Reduce Memory

1. Activation Checkpoint, Compression
2. All Work in parallel with ZeRO
3. Requires CPU-GPU-CPU transfer
4. Maintaining coarser-grained stats of model params and gradients

* Zero - DDP
![alt text](img/cmu-11868-3/image-22.png)
#### ZeRO 1: Partitioning Optimizer States
* forward pass to produce activations and loss (by fp16 parameters)
![alt text](img/cmu-11868-3/image-23.png)

* forward pass to produce activations and loss (by fp16 parameters)
![alt text](img/cmu-11868-3/image-24.png)

* forward pass to produce activations and loss (by fp16 parameters)
![alt text](img/cmu-11868-3/image-25.png)

* forward pass to produce activations and loss (by fp16 parameters)
![alt text](img/cmu-11868-3/image-26.png)


* loss backward to calculate fp16 gradients
![alt text](img/cmu-11868-3/image-27.png)

* loss backward to calculate fp16 gradients
![alt text](img/cmu-11868-3/image-28.png)

* loss backward to calculate fp16 gradients
![alt text](img/cmu-11868-3/image-29.png)

* gradient gathering from another GPU and average gradient calculation
![alt text](img/cmu-11868-3/image-30.png)

* fp32 gradient update
![alt text](img/cmu-11868-3/image-31.png)

* fp32 variance update
![alt text](img/cmu-11868-3/image-32.png)

* fp32 momentum update
![alt text](img/cmu-11868-3/image-33.png)

* fp32 parameters update
![alt text](img/cmu-11868-3/image-34.png)

* copy fp32 parameters to fp16 parameters 
![alt text](img/cmu-11868-3/image-35.png)

* fp16 parameters ready
![alt text](img/cmu-11868-3/image-36.png)

* all gather the fp16 weights to complete the iteration
![alt text](img/cmu-11868-3/image-37.png)


#### ZeRO 2: Partition Gradients

* The backward pass starts

* GPU 0,1,2 hold temporary buffers for the gradients that GPU 3 is responsible for (M3) 
![alt text](img/cmu-11868-3/image-38.png)

* GPU 0,1,2 pass the M3 gradients to GPU 3
![alt text](img/cmu-11868-3/image-39.png)

* Then they delete M3 gradients, GPU 3 will keep M3 gradients
![alt text](img/cmu-11868-3/image-40.png)

* GPU 0,2,3 hold temporary buffers for the gradients that GPU 2 is responsible for (M2)

![alt text](img/cmu-11868-3/image-41.png)

* GPU 0,2,3 pass the M2 gradients to GPU 2
![alt text](img/cmu-11868-3/image-42.png)

* Then they delete M2 gradients, GPU 2 will keep M2 gradients

![alt text](img/cmu-11868-3/image-43.png)

* Same thing for GPU1/M1
![alt text](img/cmu-11868-3/image-44.png)

* Same thing for GPU0/M0
![alt text](img/cmu-11868-3/image-45.png)



#### ZeRO 3: Partitioning Parameters

* In ZeRO, model parameters are partitioned across GPUs
![alt text](img/cmu-11868-3/image-46.png)

* GPUs broadcast their parameters during forward
![alt text](img/cmu-11868-3/image-47.png)

* Parameters are discarded right after use
![alt text](img/cmu-11868-3/image-48.png)

* GPUs broadcast their parameters again during backward
![alt text](img/cmu-11868-3/image-49.png)


#### ZeRO-R


* Partitioned Activation Checkpointing
* Constant Size Buffers
* Memory Defragmentation
