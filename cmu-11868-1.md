# cmu-11868

## GPU Acceleration

### Tiling

* CUDA Device Memory Model
![alt text](img/cmu-11868/image-23.png)

* Tiling
![alt text](img/cmu-11868/image-24.png)
![alt text](img/cmu-11868/image-25.png)

1. step 1
![alt text](img/cmu-11868/image-26.png)

2. step 2
![alt text](img/cmu-11868/image-27.png)
3. code
![alt text](img/cmu-11868/image-28.png)


* Memory Restriction
![alt text](img/cmu-11868/image-29.png)


### Memory parallelism  & Accelerating Matrix Multiplication on GPU
* Locality / Bursts Organization
![alt text](img/cmu-11868/image-30.png)

* Coalesced Access
![alt text](img/cmu-11868/image-31.png)


* Memory Parallelism

Interleaved Data Distribution
![alt text](img/cmu-11868/image-37.png)

![alt text](img/cmu-11868/image-38.png)

### Sparse Matrix - CSR
![alt text](img/cmu-11868/image-32.png)

![alt text](img/cmu-11868/image-33.png)

![alt text](img/cmu-11868/image-34.png)

### Convolutional Neural Network
![alt text](img/cmu-11868/image-35.png)

![alt text](img/cmu-11868/image-36.png)




## Accelerating Transformer Training and Inference

LightSeq            --- NAACL 2021
TurboTransformers   --- PPoPP 2021
LightSeq2           --- scss  2022
TensorRT-LLM

* Transformer 通用架构
CLIP
BERT
Stable Diffision
Vision Transformer, Swin-Transformer
wav2vec, HuBERT

* 其他架构
Alternative Model Structures: Linformer, Reformer
Training Strategy: Shallow to Deep, Layer Dropout
Efficient Computation: LAMB, Quantization, Hardware Optimization

### LightSeq
> https://github.com/RulinShao/LightSeq

1. Kernel Fusion

2. Reduce synchronization



## Distributed GPU Training

![Alt text](img/cmu-11868/image-39.png)

### NCCL
1. PCIe
2. NVLink
3. InfiniBand
4. IP sockets

* Broadcast
![Alt text](img/cmu-11868/image-40.png)
* Reduce
![Alt text](img/cmu-11868/image-41.png)
* ReduceScatter
![Alt text](img/cmu-11868/image-43.png)
* AllGather
![Alt text](img/cmu-11868/image-44.png)
* AllReduce = Reduce + Broadcast
![Alt text](img/cmu-11868/image-42.png)

---
* Point-to-Point Communication
![Alt text](img/cmu-11868/image-45.png)


### Distributed Data Parallel
> DP遗留问题
> 1）梯度的allreduce是backward完全结束后进行的：这导致gpu的计算和网络的通信变为两个串行的操作，但其实他们是可以做到overlap的。
> 2）针对每个梯度都发起一次all reduce：当梯度较小且模型又比较大时，all reduce的通信性能会变得很差，因为小数据量的通信无法充分利用通信带宽。


#### Constructor
> torch.nn.parallel.DistributedDataParallel __init__()
* rank 0将model的parameter和buffer 广播给其他所有rank，保证所有worker以相同的初始化状态进行训练;
* pytorch中将grad进行分组（一个组被称为一个bucket），然后以bucket的粒度来进行collective communication
* 遍历所有parameter，为每个parameter添加autograd hook

#### Forward
> torch.nn.parallel.DistributedDataParallel.forward()
* 调用model 的forward()进行前向计算
* 遍历所有parameter，标记unused parameter，表示这个parameter并不会进行梯度计算，这样在autograd_hook中会认为这个parameter已经处于ready状态了；

#### Autograd_hook
* autograd_hook在param计算完grad后被调用

* autograd_hook都会标记当前param已经是ready了
* 若当前autograd_hook 其对应的bucket已经处于ready状态了，则会调用collective communication接口进行all reduce操作
![Alt text](img/cmu-11868/image-47.png)


#### Pytorch 实现gradient bucketing
> Pytorch在代码层面是如何实现gradient bucketing
* 按照param的size和bucket的limit size，将param分配给bucket；
* 根据param的分配结果，创建bucket
![Alt text](img/cmu-11868/image-46.png)

#### Gradient Reduction
> gradient reduction前面已经有过简短的介绍，其目的是在所有rank计算完local的grad后，在rank间对grad进行allreduce同步。Pytorch中grad reduction实现是通过tensor的hook机制在实现的，hook function会在tensor的grad计算完成后被调用，并将新的grad值赋给param的grad field。

![Alt text](img/cmu-11868/image-48.png)


### Model Parallel

![Alt text](img/cmu-11868/image-49.png)

### Pipeline Parallelism
![Alt text](img/cmu-11868/image-50.png)


