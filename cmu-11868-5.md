
## vllm

![Alt text](img/cmu-11868-5/image-1.png)

### PagedAttention
> KV cache is split into multiple blocks
> Each block contains key and value vector for a fixed number of tokens

![Alt text](img/cmu-11868-5/image-2.png)

* KV Cache Manager
> KV Blocks = OS Pages
> Logical KV Blocks = Virtual Pages
> Physical KV Blocks = Physical Pages
> Tokens = Bytes
> Requests = Processes

![Alt text](img/cmu-11868-5/image-3.png)

![Alt text](img/cmu-11868-5/image-4.png)


#### Decoding with PagedAttention and vLLM
![Alt text](img/cmu-11868-5/image-5.png)
![Alt text](img/cmu-11868-5/image-6.png)
![Alt text](img/cmu-11868-5/image-7.png)
![Alt text](img/cmu-11868-5/image-8.png)
![Alt text](img/cmu-11868-5/image-9.png)

#### Application to Other Decoding Scenarios
* Parallel Sampling
![Alt text](img/cmu-11868-5/image-10.png)
![Alt text](img/cmu-11868-5/image-11.png)
![Alt text](img/cmu-11868-5/image-12.png)
![Alt text](img/cmu-11868-5/image-13.png)
![Alt text](img/cmu-11868-5/image-14.png)

* Beam Search
![Alt text](img/cmu-11868-5/image-15.png)

* Shared Prefix
![Alt text](img/cmu-11868-5/image-16.png)

## JAX
> Compiling machine learning programs via high-level tracing

* Automatic Differentiation
* Functional Programming
* Interoperability with NumPy
* XLA Compilation
* 4 main transformations
  * grad(): automatically differentiate a function
  * vmap(): automatically vectorize operations
  * pmap(): parallel computation of SPMD programs
  * jit(): transform a function into a JIT-compiled version

### Functional Programming
![alt text](img/cmu-11868-5/image-17.png)

* Transformations: grad()
![alt text](img/cmu-11868-5/image-18.png)

* Transformations: vmap()
![alt text](img/cmu-11868-5/image-19.png)

* Transformations: pmap()
![alt text](img/cmu-11868-5/image-20.png)
* Transformations: jit()
![alt text](img/cmu-11868-5/image-21.png)

### Jaxpr
![alt text](img/cmu-11868-5/image-22.png)

* example
![alt text](img/cmu-11868-5/image-23.png)

![alt text](img/cmu-11868-5/image-24.png)

![alt text](img/cmu-11868-5/image-25.png)

* JAX tracing with Jaxprs: XLA_call
![alt text](img/cmu-11868-5/image-26.png)

### JAX

* Introduction
1. Just-in-time (JIT) compiler
2. Convert pure Python and Numpy into high-performance code
3. Run efficiently on various accelerators (CPUs, GPUs, TPUs)
4. Write easily with Python while achieving significant speedups

![alt text](img/cmu-11868-5/image-27.png)
![alt text](img/cmu-11868-5/image-28.png)

* Method
![alt text](img/cmu-11868-5/image-29.png)

* Design
![alt text](img/cmu-11868-5/image-30.png)

* Operator Fusion Mechanisms in JAX (XLA)

![alt text](img/cmu-11868-5/image-31.png)

* Instruction Fusion
![alt text](img/cmu-11868-5/image-32.png)

* Fusion Merger
![alt text](img/cmu-11868-5/image-33.png)

* Sibling Fusion
![alt text](img/cmu-11868-5/image-34.png)

* Producer-consumer Fusion
![alt text](img/cmu-11868-5/image-35.png)


